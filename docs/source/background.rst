Background and Motivation
=========================

**Data Transfer Framework (DTF)** is an I/O middleware designed for achieving scalable and high-speed data transfer between application components in multi-component workflows. 
Multi-component workflow is becoming a popular computation model in High Performance Computing (HPC), in which multiple independently developed application components are coupled together to perform more sophisticated and complex computations (e.g. data assimilation based weather prediction system).

In such workflows, massive computation data generated by a component during execution need to be transferred to the other as the input data to the subsequent computations.
However, the implementations of inter-component data exchange in most of the multi-compoenent systems are either based on file I/O through parallel file system or specifically designed coupling softwares.
Both of the approaches have pros and cons.
Due to the fact that coupled application components are usually developed by different research teams, file I/O became the easiest approach, which exchanges data through file systems using high-performance parallel I/O libraries (e.g. PnetCDF and HDF5).
The major drawbacks of file I/O based approach are low speed and inefficiency because the reader components have to wait until the writer component finishes its data writing process.
Coupling softwares can be deployed to overcome the mentioned shortcomings of file I/O based data transfer.
However, the I/O kernels of existent multi-component projects are required to be largely modified using the dedicated Application Programming Interfaces (APIs) provided by these coupling softwares.


It is implemented using Message Passing Interface (MPI), which transparently redirects PnetCDF file I/O operations to DTF library implementations.

.. _fileio-dtf:

.. figure:: fileio-dtf.png
    :scale: 60%
    :align: center
    
    Difference between File I/O based and DTF based inter-component data exchange
